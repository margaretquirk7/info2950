{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f77090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import statsmodels.api as sm\n",
    "import duckdb, sqlalchemy\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7ec0",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b3ddd",
   "metadata": {},
   "source": [
    "We obtained our data through 3 data requests to weather.gov. First, we requested Ithaca's data, and then we decided to expand out analysis to include locations north, east, south, and west of Ithaca. Our second data pull included the following cities: Watertown (North), Bloomsburg (South), Cobleskill (East), and Avoca (West). Avoca did not contain any temperature data, so we decided to request data for Erie as a replacement. The cell below, reads in the csv files obtained from weather.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a4db0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_25496\\2358469234.py:2: DtypeWarning: Columns (9,15,17,19,21,27,29,31,33,35,37,39,41,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ithaca = pd.read_csv(\"Ithaca.csv\")\n",
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_25496\\2358469234.py:3: DtypeWarning: Columns (27,29,31,33,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  others = pd.read_csv(\"AdjacentCities.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORTH EAST 1.2 WNW, PA US' 'ERIE 4.4 ESE, PA US' 'ERIE 5.7 SSE, PA US'\n",
      " 'ERIE 5.6 SW, PA US' 'NORTH EAST 2.0 S, PA US' 'ERIE 5.3 E, PA US'\n",
      " 'ERIE INTERNATIONAL AIRPORT, PA US']\n",
      "['BLACK RIVER, NY US' 'BLOOMSBURG UNIVERSITY, PA US'\n",
      " 'COBLESKILL 2 ESE, NY US' 'COBLESKILL 4.2 NNE, NY US'\n",
      " 'COBLESKILL 5.7 W, NY US' 'ESPY 0.8 S, PA US' 'WATERTOWN 0.2 SSE, NY US'\n",
      " 'WATERTOWN 1.0 ESE, NY US' 'WATERTOWN 1.3 WNW, NY US'\n",
      " 'WATERTOWN 7.0 N, NY US' 'WATERTOWN, NY US']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_25496\\2358469234.py:4: DtypeWarning: Columns (7,9,11,13,15,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  west = pd.read_csv(\"Erie.csv\")\n"
     ]
    }
   ],
   "source": [
    "#read in csvs\n",
    "ithaca = pd.read_csv(\"Ithaca.csv\")\n",
    "others = pd.read_csv(\"AdjacentCities.csv\")\n",
    "west = pd.read_csv(\"Erie.csv\")\n",
    "#print(ithaca.head())\n",
    "print(west['NAME'].unique())\n",
    "others = others.dropna(axis=0,subset=['STATION'])\n",
    "print(others['NAME'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915028f7",
   "metadata": {},
   "source": [
    "Since certain locations contained multiple weather stations, we created the column Location to group the entries as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752d020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['North' 'South' 'East']\n"
     ]
    }
   ],
   "source": [
    "#Add column to indicate location (relative to ithaca)\n",
    "ithaca[\"Location\"] = \"Central\"\n",
    "west['Location'] = \"West\"\n",
    "#print(others['NAME'].unique())\n",
    "locations = []\n",
    "for i in others['NAME']:\n",
    "    if \"WATERTOWN\" in i:\n",
    "        locations.append('North')\n",
    "    elif \"BLACK RIVER\" in i:\n",
    "        locations.append(\"North\")\n",
    "    elif \"ESPY\" in i:\n",
    "        locations.append(\"South\")\n",
    "    elif \"BLOOMSBURG\" in i:\n",
    "        locations.append(\"South\")\n",
    "    elif \"COBLESKILL\" in i:\n",
    "        locations.append(\"East\")\n",
    "    else:\n",
    "        locations.append('N/A')\n",
    "others['Location'] = locations\n",
    "print(others['Location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74e4c",
   "metadata": {},
   "source": [
    "Certain cities had more data attributes available than others, so to maintain cohesion, the cell below identifies common columns between the three data sets. We then dropped any uncommon columns and concatenated the three datasets to form one dataframe containing all relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd20e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns match: proceed\n"
     ]
    }
   ],
   "source": [
    "#Check all columns match before concatenation\n",
    "columns_to_keep = []\n",
    "\n",
    "for i in ithaca.columns.tolist():\n",
    "    if (i in others.columns.tolist()) & (i in west.columns.tolist()) :\n",
    "        columns_to_keep.append(i)\n",
    "ithaca_good = ithaca[columns_to_keep]\n",
    "others_good = others[columns_to_keep]\n",
    "west_good = west[columns_to_keep]\n",
    "        \n",
    "if (ithaca_good.columns.tolist() == others_good.columns.tolist()) & (ithaca_good.columns.tolist() == west_good.columns.tolist()):\n",
    "    print(\"Columns match: proceed\")\n",
    "    final_df = pd.concat([ithaca_good,others_good,west_good])\n",
    "else:\n",
    "    print(ithaca.columns)\n",
    "    print(others.columns)\n",
    "    \n",
    "#print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952014f8",
   "metadata": {},
   "source": [
    "In the cell below, we checked to make sure that none of the columns contained only null values. All columns contained at least one relevant datapoint, but if this was not the case, we also included code that would drop an entirely null column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec49211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'DATE', 'DAPR',\n",
      "       'DAPR_ATTRIBUTES', 'MDPR', 'MDPR_ATTRIBUTES', 'PRCP', 'PRCP_ATTRIBUTES',\n",
      "       'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TMAX',\n",
      "       'TMAX_ATTRIBUTES', 'TMIN', 'TMIN_ATTRIBUTES', 'WESD', 'WESD_ATTRIBUTES',\n",
      "       'WESF', 'WESF_ATTRIBUTES', 'WT05', 'WT05_ATTRIBUTES', 'Location'],\n",
      "      dtype='object')\n",
      "No null columns\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in columns\n",
    "print(final_df.columns)\n",
    "#If a column has only null values, drop the column\n",
    "null_cols = []\n",
    "for c in final_df.columns:\n",
    "    if final_df[c].isnull().all():\n",
    "        null_cols.append(c)\n",
    "if len(null_cols) == 0:\n",
    "    print(\"No null columns\")\n",
    "else:\n",
    "    print('Null columns:', null_cols)\n",
    "    final_df = final_df.drop(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d0ed7",
   "metadata": {},
   "source": [
    "The attributes columns in the weather.gov data contains a string that contains multiple attribute indicators concatenated together. \"Trace\" is an attribute that we believe will be relevant to our analysis and is represented by a \"T\" in the attribute columns. We used this to create new binary columns that indicate whether or not there was a trace of precipitation or snow on a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32ee2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary columns to indicate if there was trace of precipitation for snow and rain\n",
    "precip_binary = []\n",
    "snow_binary = []\n",
    "for p in final_df['PRCP_ATTRIBUTES']:\n",
    "    #print(type(p))\n",
    "    if (type(p) == str):\n",
    "        if 'T' in p:\n",
    "            precip_binary.append(1)\n",
    "        else:\n",
    "            precip_binary.append(0)\n",
    "    else:\n",
    "        precip_binary.append(0)\n",
    "        \n",
    "for s in final_df['SNOW_ATTRIBUTES']:\n",
    "    if (type(s) == str):\n",
    "        if 'T' in s:\n",
    "            snow_binary.append(1)\n",
    "        else:\n",
    "            snow_binary.append(0)\n",
    "    else:\n",
    "        snow_binary.append(0)\n",
    "    \n",
    "final_df['PrecipTrace'] = precip_binary\n",
    "final_df['SnowTrace'] = snow_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42f81a",
   "metadata": {},
   "source": [
    "With the addition of the binary columns, we no longer need the initial attributes columns and drop them from the dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3a3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attributes columns now that we have created the binary columns\n",
    "final_df = final_df.drop(columns = ['PRCP_ATTRIBUTES', 'SNOW_ATTRIBUTES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7c344",
   "metadata": {},
   "source": [
    "In addition, we converted the data column to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b0a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date column to datetime\n",
    "final_df['DATE'] = pd.to_datetime(final_df['DATE'], format = '%m/%d/%Y')\n",
    "#print(final_df['DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172c692",
   "metadata": {},
   "source": [
    "We also created a column that indicates the season. We categorized seasons as:\n",
    "\n",
    "Winter: December-February\n",
    "\n",
    "Spring: March-May\n",
    "\n",
    "Summer: June-August\n",
    "\n",
    "Fall: September-November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f7be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_list = []\n",
    "for i in final_df['DATE']:\n",
    "    if (i.month == 12) or (i.month == 1) or (i.month == 2):\n",
    "        season_list.append('Winter')\n",
    "    elif (i.month == 3) or (i.month == 4) or (i.month == 5):\n",
    "        season_list.append('Spring')\n",
    "    elif (i.month == 6) or (i.month == 7) or (i.month == 8):\n",
    "        season_list.append('Summer')\n",
    "    elif (i.month == 9) or (i.month == 10) or (i.month == 11):\n",
    "        season_list.append('Fall')\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "final_df['Season'] = season_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54a7e2",
   "metadata": {},
   "source": [
    "In some cases, there were multiple stations within the same location category that recorded data. To account for this, we created an aggregated dataframe that groups by location and date and takes the average of the temperature, location, elevation, and precipitation data. This aggregated dataframe takes the maximum of the binary columns to indicate if there was a trace anywhere within the location. We wanted to maintain the binary property of the columns in order to perform logistic regression in the final phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbdc6d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data to local variable agg_df\n"
     ]
    }
   ],
   "source": [
    "#Aggregate dataframe to account for locations with multiple stations taking recordings on the same day\n",
    "%sql agg_df << select DATE, Location, avg(LATITUDE) as Latitude, avg(LONGITUDE) as Longitude, avg(ELEVATION) as Elevation, AVG(TMAX) as MaxTemp, AVG(TMIN) as MinTemp,AVG(PRCP) as Precipitation, AVG(SNOW) as Snowfall, MAX(PrecipTrace) as PrecipTrace, MAX(SnowTrace) as SnowTrace, Season from final_df group by Location, DATE, Season order by DATE\n",
    "agg_df['AverageTemp'] = 0.5*agg_df['MaxTemp'] + 0.5*agg_df['MinTemp']\n",
    "\n",
    "agg_df['Month_Grouping'] = agg_df['DATE'].dt.to_period('M')\n",
    "agg_df['Month_Grouping'] = agg_df['Month_Grouping'].astype('datetime64[M]')\n",
    "#print(agg_df[['DATE','Month Grouping']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b8888",
   "metadata": {},
   "source": [
    "We also split the aggregated dataframe into smaller dataframes based on location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278b1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 5 separate data frames based on location for individual analyses when needed\n",
    "central = agg_df[agg_df['Location']==\"Central\"]\n",
    "north = agg_df[agg_df['Location']==\"North\"]\n",
    "south = agg_df[agg_df['Location']==\"South\"]\n",
    "east = agg_df[agg_df['Location']==\"East\"]\n",
    "west = agg_df[agg_df['Location']==\"West\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31166c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export agg_df\n",
    "agg_df.to_csv('agg_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
