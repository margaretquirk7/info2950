{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1937c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import duckdb, sqlalchemy\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d540d",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. Is there a significant difference between historical weather trends when moving about 100 miles away from a central location?\n",
    "2. Is it possible to accurately model and predict the weather based on another locationâ€™s weather?\n",
    "3. Does latitude, longitude, or elevation have the most significant impact on temperature? On precipitation?\n",
    "4. How does proximity to a lake impact historical trends? Is there a significant difference between the variation in temperature or precipitation for Erie and Ithaca compared to the other locations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df2eeb",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab252cb0",
   "metadata": {},
   "source": [
    "We obtained our data through 3 data requests to weather.gov. First, we requested Ithaca's data, and then we decided to expand out analysis to include locations north, east, south, and west of Ithaca. Our second data pull included the following cities: Watertown (North), Bloomsburg (South), Cobleskill (East), and Avoca (West). Avoca did not contain any temperature data, so we decided to request data for Erie as a replacement. The cell below, reads in the csv files obtained from weather.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8b9d6af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_18428\\1485939667.py:2: DtypeWarning: Columns (9,15,17,19,21,27,29,31,33,35,37,39,41,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ithaca = pd.read_csv(\"Ithaca.csv\")\n",
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_18428\\1485939667.py:3: DtypeWarning: Columns (27,29,31,33,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  others = pd.read_csv(\"AdjacentCities.csv\")\n",
      "C:\\Users\\Margaret Quirk\\AppData\\Local\\Temp\\ipykernel_18428\\1485939667.py:4: DtypeWarning: Columns (7,9,11,13,15,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  west = pd.read_csv(\"Erie.csv\")\n"
     ]
    }
   ],
   "source": [
    "#read in csvs\n",
    "ithaca = pd.read_csv(\"Ithaca.csv\")\n",
    "others = pd.read_csv(\"AdjacentCities.csv\")\n",
    "west = pd.read_csv(\"Erie.csv\")\n",
    "#print(ithaca.head())\n",
    "#print(west.head())\n",
    "others = others.dropna(axis=0,subset=['STATION'])\n",
    "#print(others.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54359ac2",
   "metadata": {},
   "source": [
    "Since certain locations contained multiple weather stations, we created the column Location to group the entries as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7d0bd2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['North' 'South' 'East']\n"
     ]
    }
   ],
   "source": [
    "#Add column to indicate location (relative to ithaca)\n",
    "ithaca[\"Location\"] = \"Central\"\n",
    "west['Location'] = \"West\"\n",
    "#print(others['NAME'].unique())\n",
    "locations = []\n",
    "for i in others['NAME']:\n",
    "    if \"WATERTOWN\" in i:\n",
    "        locations.append('North')\n",
    "    elif \"BLACK RIVER\" in i:\n",
    "        locations.append(\"North\")\n",
    "    elif \"ESPY\" in i:\n",
    "        locations.append(\"South\")\n",
    "    elif \"BLOOMSBURG\" in i:\n",
    "        locations.append(\"South\")\n",
    "    elif \"COBLESKILL\" in i:\n",
    "        locations.append(\"East\")\n",
    "    else:\n",
    "        locations.append('N/A')\n",
    "others['Location'] = locations\n",
    "print(others['Location'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff0bb9",
   "metadata": {},
   "source": [
    "Certain cities had more data attributes available than others, so to maintain cohesion, the cell below identifies common columns between the three data sets. We then dropped any uncommon columns and concatenated the three datasets to form one dataframe containing all relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5602e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns match: proceed\n"
     ]
    }
   ],
   "source": [
    "#Check all columns match before concatenation\n",
    "columns_to_keep = []\n",
    "\n",
    "for i in ithaca.columns.tolist():\n",
    "    if (i in others.columns.tolist()) & (i in west.columns.tolist()) :\n",
    "        columns_to_keep.append(i)\n",
    "ithaca_good = ithaca[columns_to_keep]\n",
    "others_good = others[columns_to_keep]\n",
    "west_good = west[columns_to_keep]\n",
    "        \n",
    "if (ithaca_good.columns.tolist() == others_good.columns.tolist()) & (ithaca_good.columns.tolist() == west_good.columns.tolist()):\n",
    "    print(\"Columns match: proceed\")\n",
    "    final_df = pd.concat([ithaca_good,others_good,west_good])\n",
    "else:\n",
    "    print(ithaca.columns)\n",
    "    print(others.columns)\n",
    "    \n",
    "#print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e3818",
   "metadata": {},
   "source": [
    "In the cell below, we checked to make sure that none of the columns contained only null values. All columns contained at least one relevant datapoint, but if this was not the case, we also included code that would drop an entirely null column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "34c7edb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'DATE', 'DAPR',\n",
      "       'DAPR_ATTRIBUTES', 'MDPR', 'MDPR_ATTRIBUTES', 'PRCP', 'PRCP_ATTRIBUTES',\n",
      "       'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TMAX',\n",
      "       'TMAX_ATTRIBUTES', 'TMIN', 'TMIN_ATTRIBUTES', 'WESD', 'WESD_ATTRIBUTES',\n",
      "       'WESF', 'WESF_ATTRIBUTES', 'WT05', 'WT05_ATTRIBUTES', 'Location'],\n",
      "      dtype='object')\n",
      "No null columns\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in columns\n",
    "print(final_df.columns)\n",
    "#If a column has only null values, drop the column\n",
    "null_cols = []\n",
    "for c in final_df.columns:\n",
    "    if final_df[c].isnull().all():\n",
    "        null_cols.append(c)\n",
    "if len(null_cols) == 0:\n",
    "    print(\"No null columns\")\n",
    "else:\n",
    "    print('Null columns:', null_cols)\n",
    "    final_df = final_df.drop(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3da4e",
   "metadata": {},
   "source": [
    "The attributes columns in the weather.gov data contains a string that contains multiple attribute indicators concatenated together. \"Trace\" is an attribute that we believe will be relevant to our analysis and is represented by a \"T\" in the attribute columns. We used this to create new binary columns that indicate whether or not there was a trace of precipitation or snow on a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a8444027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary columns to indicate if there was trace of precipitation for snow and rain\n",
    "precip_binary = []\n",
    "snow_binary = []\n",
    "for p in final_df['PRCP_ATTRIBUTES']:\n",
    "    #print(type(p))\n",
    "    if (type(p) == str):\n",
    "        if 'T' in p:\n",
    "            precip_binary.append(1)\n",
    "        else:\n",
    "            precip_binary.append(0)\n",
    "    else:\n",
    "        precip_binary.append(0)\n",
    "        \n",
    "for s in final_df['SNOW_ATTRIBUTES']:\n",
    "    if (type(s) == str):\n",
    "        if 'T' in s:\n",
    "            snow_binary.append(1)\n",
    "        else:\n",
    "            snow_binary.append(0)\n",
    "    else:\n",
    "        snow_binary.append(0)\n",
    "    \n",
    "final_df['PrecipTrace'] = precip_binary\n",
    "final_df['SnowTrace'] = snow_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398b56b",
   "metadata": {},
   "source": [
    "With the addition of the binary columns, we no longer need the initial attributes columns and drop them from the dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "de182c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attributes columns now that we have created the binary columns\n",
    "final_df = final_df.drop(columns = ['PRCP_ATTRIBUTES', 'SNOW_ATTRIBUTES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b90bac",
   "metadata": {},
   "source": [
    "In addition, we converted the data column to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f1758284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date column to datetime\n",
    "final_df['DATE'] = pd.to_datetime(final_df['DATE'], format = '%m/%d/%Y')\n",
    "#print(final_df['DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83bd41e",
   "metadata": {},
   "source": [
    "In some cases, there were multiple stations within the same location category that recorded data. To account for this, we created an aggregated dataframe that groups by location and date and takes the average of the temperature, location, elevation, and precipitation data. This aggregated dataframe takes the maximum of the binary columns to indicate if there was a trace anywhere within the location. We wanted to maintain the binary property of the columns in order to perform logistic regression in the final phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5df8901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data to local variable agg_df\n",
      "            DATE Location   Latitude  Longitude  Elevation  MaxTemp  MinTemp  \\\n",
      "0     2010-02-11     West  42.104302 -80.054233    271.325     25.0     19.0   \n",
      "1     2010-02-12     West  42.104302 -80.054233    271.325     25.0     17.0   \n",
      "2     2010-03-01     West  42.104302 -80.054233    271.325     34.0     29.0   \n",
      "3     2010-03-02     West  42.104302 -80.054233    271.325     31.0     27.0   \n",
      "4     2010-03-18     West  42.104302 -80.054233    271.325     59.0     34.0   \n",
      "...          ...      ...        ...        ...        ...      ...      ...   \n",
      "34889 2008-05-13    North  43.976100 -75.875300    151.500     68.0     41.0   \n",
      "34890 2007-09-25    North  43.976100 -75.875300    151.500      NaN      NaN   \n",
      "34891 2007-09-07    North  43.976100 -75.875300    151.500      NaN      NaN   \n",
      "34892 2010-04-05    North  43.976100 -75.875300    151.500     63.0     45.0   \n",
      "34893 2010-12-28    North  43.976100 -75.875300    151.500     19.0      0.0   \n",
      "\n",
      "       Precipitation  Snowfall  PrecipTrace  SnowTrace  \n",
      "0             0.1550     2.775            1          0  \n",
      "1             0.0125     0.225            1          1  \n",
      "2             0.0325     0.475            0          0  \n",
      "3             0.0000     0.000            0          0  \n",
      "4             0.0000     0.000            0          0  \n",
      "...              ...       ...          ...        ...  \n",
      "34889         0.0000     0.000            0          0  \n",
      "34890         0.0000     0.000            0          0  \n",
      "34891         0.0000     0.000            0          0  \n",
      "34892         0.0000     0.000            0          0  \n",
      "34893         0.0000     0.000            0          0  \n",
      "\n",
      "[34894 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#Aggregate dataframe to account for locations with multiple stations taking recordings on the same day\n",
    "%sql agg_df << select DATE, Location, avg(LATITUDE) as Latitude, avg(LONGITUDE) as Longitude, avg(ELEVATION) as Elevation, AVG(TMAX) as MaxTemp, AVG(TMIN) as MinTemp,AVG(PRCP) as Precipitation, AVG(SNOW) as Snowfall, MAX(PrecipTrace) as PrecipTrace, MAX(SnowTrace) as SnowTrace from final_df group by Location, Date\n",
    "print(agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f7527",
   "metadata": {},
   "source": [
    "We also split the aggregated dataframe into smaller dataframes based on location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c0c40c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 5 separate data frames based on location for individual analyses when needed\n",
    "central = agg_df[agg_df['Location']==\"Central\"]\n",
    "north = agg_df[agg_df['Location']==\"North\"]\n",
    "south = agg_df[agg_df['Location']==\"South\"]\n",
    "east = agg_df[agg_df['Location']==\"East\"]\n",
    "west = agg_df[agg_df['Location']==\"West\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec418f",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80aaa8",
   "metadata": {},
   "source": [
    "## Data Limitations\n",
    "\n",
    "Missing values: The most consistent quantitative observations available for each location and date are temperature, elevation, precipitation, and snowfall. However, there are many other columns containing other types of observations, which have a significant amount of null values. This may make it hard to incorporate these other observations if there is not enough data, and could lead to a less accurate model.\n",
    "\n",
    "Different length of time frames: Five cities are included in this analysis and new dataset, however different amounts of data exist for each city. Ideally 20 years of history would be included for each city, however this was not available, so there is less data to use to train a model for certain cities, which could impact accuracy of results.\n",
    "\n",
    "Limited scope of cities: Only a total of 5 cities are included in this dataset, so conclusions from this analysis are not necessarily applicable to other areas. This is especially true because this dataset is centered around Ithaca, which is a Northeastern climate, however in a desert setting the results to these research questions likely will not be applicable based on this analysis.\n",
    "\n",
    "Attributes are not very descriptive: The attributes for different weather events are not very descriptive. This could make it difficult to incorporate the attribute columns into an analysis as it is unclear what some of the different attributes mean, especially in the context of the research questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626b02f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d976a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083c2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#side by side linear regression of consumption vs grades for math and portuguese sets\n",
    "#histogram for grades/consumption/absences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
